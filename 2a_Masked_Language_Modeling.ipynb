{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fSK_V_AGmV_"
   },
   "source": [
    "# BERT Masked Language Modeling\n",
    "\n",
    "This notebook demonstrates the masked language modeling (MLM) task using the BERT model. MLM is a technique for training language models to predict masked words in a sentence. BERT is a powerful pre-trained language model that can be fine-tuned for various NLP tasks, including MLM.\n",
    "\n",
    "This tutorial is based on Eyal Gruss's own [tutorial](https://github.com/eyaler/workshop/blob/master/bert.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "neuYpOFMGThV"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.31.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNz99ZQMGUiC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjPAU8UgG0SK"
   },
   "source": [
    "## Loading the Model and Tokenizer\n",
    "\n",
    "This section loads the pre-trained BERT model and tokenizer. We use the `bert-base-uncased` variant of BERT, which is a smaller and faster version of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d0txjTvSGVfY"
   },
   "outputs": [],
   "source": [
    "# Init tokenizer, BERT uses its own word part tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model (weights) and set to evaluation mode\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# Move model to CUDA if available\n",
    "try:\n",
    "    model = model.to('cuda')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucHmn8yxG65n"
   },
   "source": [
    "## Running Masked Language Modeling\n",
    "\n",
    "This section shows how to run masked language modeling using the `predict_word` function.\n",
    "\n",
    "### The `predict_word` Function\n",
    "\n",
    "The `predict_word` function takes a text input with a masked token (`[MASK]`) and predicts the most likely words to fill the mask. It uses the BERT model and tokenizer to process the input and generate predictions.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "* `text`: A sentence containing exactly one `[MASK]` token to predict. For example: 'Alex likes to have [MASK] with his best friend'.\n",
    "* `model`: A `BertForMaskedLM` model.\n",
    "* `tokenizer`: A BERT tokenizer.\n",
    "* `topn`: The number of candidates to return (default: 10).\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "* A list of tuples, where each tuple contains a predicted token and its probability.\n",
    "\n",
    "**Example Usage:**\n",
    "\n",
    "\n",
    "```\n",
    "predictions = predict_word('The boy [MASK] to the school', model, tokenizer)\n",
    "print(predictions)\n",
    "```\n",
    "\n",
    "This will print the top 10 predicted words and their probabilities for the masked token in the sentence \"The boy [MASK] to the school\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "haG71uEdGYZ1"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the best matching guesses for the masked word\n",
    "text: a sentence much include exactly one [MASK] token to predict.\n",
    "      For example:\n",
    "      'Alex likes to have [MASK] with his best friend'\n",
    "model: a BertForMaskedLM\n",
    "tokenizer: a Bert tokenizer\n",
    "topn: Number of candidates for mask\n",
    "\n",
    "Returns candidates and their probs\n",
    "\"\"\"\n",
    "def predict_word(text, model, tokenizer, topn=10):\n",
    "  # Prepare text\n",
    "  text = '[CLS] '+ text.lstrip('[CLS] ').rstrip(' [SEP]')+' [SEP]'\n",
    "  # Tokenize input\n",
    "  tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "  # Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "  masked_index = -1\n",
    "  for i, token in enumerate(tokenized_text):\n",
    "    if token=='[MASK]':\n",
    "      masked_index = i\n",
    "      break\n",
    "  assert i>=0\n",
    "\n",
    "  # Convert token to vocabulary indices\n",
    "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "  # Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "  segments_ids = [0]*len(tokenized_text)\n",
    "\n",
    "  # Convert inputs to PyTorch tensors\n",
    "  tokens_tensor = torch.tensor([indexed_tokens])\n",
    "  segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "  # If you have a GPU, put everything on cuda\n",
    "  tokens_tensor = tokens_tensor.to(model.device) # Instead of hardcoding 'cuda'\n",
    "  segments_tensors = segments_tensors.to(model.device) # Instead of hardcoding 'cuda'\n",
    "\n",
    "  # Predict all tokens\n",
    "  with torch.no_grad():\n",
    "      outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "      predictions = outputs[0]\n",
    "  print(\"Predictions shape: \" + str(predictions[0].shape))\n",
    "  predicted_inds = torch.argsort(-predictions[0, masked_index])\n",
    "  predicted_probs = [round(p.item(),4) for p in torch.softmax(predictions[0, masked_index], 0)[predicted_inds]]\n",
    "  predicted_tokens = tokenizer.convert_ids_to_tokens([ind.item() for ind in predicted_inds])\n",
    "\n",
    "  return list(zip(predicted_tokens, predicted_probs))[:topn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjaLRHVu_kOi"
   },
   "source": [
    "## Masked Language Modeling with BERT\n",
    "\n",
    "This function implements masked language modeling using the BERT model. Here's a breakdown of the process:\n",
    "\n",
    "1. **Preprocessing:** The input text is preprocessed by adding the special BERT tokens `[CLS]` at the beginning and `[SEP]` at the end. These tokens are crucial for BERT's understanding of the text structure.\n",
    "2. **Tokenization and Conversion:** The text is tokenized into individual words or subwords, and these tokens are converted into their corresponding numerical IDs within BERT's vocabulary.\n",
    "3. **Prediction:** For each token, the BERT model outputs a probability distribution over its entire vocabulary, essentially predicting the most likely words to fill that position. This distribution is represented as a softmax vector with dimensions equal to the vocabulary size.\n",
    "4. **Selection:**  The function identifies the tokens with the highest probabilities, indicating the most likely candidates for the masked word.\n",
    "\n",
    "<img src='http://jalammar.github.io/images/BERT-language-modeling-masked-lm.png' width=\"600px\"/>\n",
    "\n",
    "\n",
    "(image source: [The Illustrated BERT](http://jalammar.github.io/illustrated-bert/))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3X_j9L7hbxt2"
   },
   "outputs": [],
   "source": [
    "predict_word('The boy [MASK] to the school', model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZQWt5nHpHuQ"
   },
   "outputs": [],
   "source": [
    "predict_word('My friend [MASK] is a mother', model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wxz1yC4iuO5a"
   },
   "outputs": [],
   "source": [
    "predict_word('My friend [MASK] is a programmer', model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VQBnd7nKuQaB"
   },
   "outputs": [],
   "source": [
    "predict_word('My friend [MASK] is a doctor', model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xpk-Jv4K_Fgw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
